<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ahmadrazacdx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ahmadrazacdx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-03T08:24:59+00:00</updated><id>https://ahmadrazacdx.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Synthesis of Biological and Computational Learning</title><link href="https://ahmadrazacdx.github.io/blog/2025/biological-vs-computational-learning/" rel="alternate" type="text/html" title="A Synthesis of Biological and Computational Learning"/><published>2025-12-02T00:00:00+00:00</published><updated>2025-12-02T00:00:00+00:00</updated><id>https://ahmadrazacdx.github.io/blog/2025/biological-vs-computational-learning</id><content type="html" xml:base="https://ahmadrazacdx.github.io/blog/2025/biological-vs-computational-learning/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>This article examines the biological substrates of learning and memory, tracing their influence on the development of artificial neural networks. We first distinguish between three commonly conflated cognitive constructs learning, thinking, and intelligence, establishing precise definitions grounded in neuroscience and cognitive psychology. We then review the principal mechanisms of synaptic plasticity: Hebbian learning, Long-Term Potentiation, Long-Term Depression, and Spike-Timing-Dependent Plasticity. The article subsequently traces how these biological principles informed computational approaches, from early artificial neurons through modern Large Language Models. We also survey counter-intuitive findings in memory research, including reconsolidation, the protégé effect, curiosity-enhanced encoding, sleep-dependent consolidation, and adult neurogenesis. The article concludes by examining the Information Bottleneck framework as a unifying principle connecting biological and artificial learning systems.</p> <h2 id="1-learning-thinking-and-intelligence">1. Learning, Thinking, and Intelligence</h2> <p>These three words learning, thinking, and intelligence get thrown around interchangeably. Your teacher says you need to “think harder”. Your friend claims they “learned” something by watching a TikTok. But in cognitive science, these refer to fundamentally different operations. Understanding the distinction is the first step to understanding your own mind.</p> <h3 id="learning-the-acquisition">Learning: The Acquisition</h3> <p>Learning is the process of acquiring new information and encoding it into memory. It is an action that changes your brain’s physical state.</p> <p>When you learn something, you are not simply adding data to a mental hard drive. You are rewiring neural circuits. Neurons form new connections called synapses, or strengthen existing ones. This physical reshaping of brain tissue is called <strong>neuroplasticity</strong>, and it is the biological foundation of all learning.</p> <p>The goal of learning is to move information from fleeting, short-term experience into durable, long-term storage. Memorizing the rules of chess is learning. Practicing how pieces move is learning. The brain is literally changing shape to accommodate this new information.</p> <h3 id="thinking-the-processing">Thinking: The Processing</h3> <p>Thinking is what you do with what you’ve learned. It is the mental manipulation of information, reasoning through a problem, weighing options, imagining outcomes, generating new ideas or insights from existing knowledge.</p> <p>While learning is about input, thinking is about processing. It relies heavily on the <strong>prefrontal cortex</strong>, the brain region behind your forehead that manages attention, retrieves memories on demand, and simulates possible futures (Miller &amp; Cohen, 2001). When you analyze a chess board, predict your opponent’s strategy, and decide your next move, you are thinking. You are using learned information to navigate a present challenge.</p> <h3 id="intelligence-the-capacity">Intelligence: The Capacity</h3> <p>If learning is fuel and thinking is driving, intelligence is the engine. It represents the efficiency and potential with which your brain can learn and think.</p> <p>Intelligence is often linked to <strong>neural efficiency</strong>, the observation that highly capable brains frequently solve complex problems using less energy, because their neural networks are better integrated and communicate faster (Haier et al., 1988). It’s why some people grasp chess strategy in an afternoon while others struggle for months. It’s not just effort; it’s the underlying hardware doing more with less.</p> <p>These distinctions matter because they reveal that being “smart” is not one thing. You can be a fast learner but a sloppy thinker. You can have high potential but never fill your tank with knowledge. Understanding this prepares you for what comes next: the biological mechanisms that make these processes possible.</p> <h2 id="2-biological-mechanisms-of-learning">2. Biological Mechanisms of Learning</h2> <p>In 1949, a Canadian psychologist Donald Hebb proposed an idea so powerful it still dominates neuroscience today. He wrote: <em>“When an axon of cell A is near enough to excite cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased”</em> (Hebb, 1949). The simplified version became a mantra: <strong>“Neurons that fire together, wire together.”</strong> This was revolutionary because it proposed that learning isn’t magic, it’s plumbing. It’s physical connections between cells getting stronger or weaker based on activity. But Hebb couldn’t prove it directly. The technology didn’t exist. It would take another 24 years for someone to catch the brain in the act.</p> <h3 id="the-save-button-long-term-potentiation-ltp">The Save Button: Long-Term Potentiation (LTP)</h3> <p>In 1973, Timothy Bliss and Terje Lømo were experimenting on rabbit brains in Norway. They discovered that when they stimulated a neural pathway with rapid, repeated signals, that pathway became hypersensitive and stayed that way for hours, even days. They called this <strong>Long-Term Potentiation</strong> (Bliss &amp; Lømo, 1973).</p> <p>LTP is your brain’s “Save” button. When you experience something important, really pay attention, get emotionally engaged, repeat it enough times, synapses don’t just fire; they structurally change. Receptors multiply. Connections thicken. The signal that once whispered now shouts. This is why cramming the night before an exam produces fragile memories that evaporate within days or weeks, while spaced practice over months creates knowledge that lasts years. LTP requires the right conditions: intensity, repetition, and time.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/ltp.webp" width="440" height="auto" alt="LTP"/> </p> <h3 id="the-delete-button-long-term-depression-ltd">The Delete Button: Long-Term Depression (LTD)</h3> <p>But if the brain only strengthened connections, it would quickly become overloaded every trivial detail preserved forever. The system needs pruning.</p> <p>In 1982, Masao Ito demonstrated the opposite of LTP in the cerebellum: <strong>Long-Term Depression</strong>, a sustained weakening of synaptic transmission (Ito &amp; Kano, 1982). LTD is how the brain corrects mistakes and clears irrelevant data. It’s the biological “Delete” button, ensuring that neural networks don’t just accumulate noise but refine themselves toward accuracy. Forgetting isn’t failure; it’s maintenance.</p> <h3 id="the-precision-spike-timing-dependent-plasticity-stdp">The Precision: Spike-Timing-Dependent Plasticity (STDP)</h3> <p>Hebb said neurons that fire together wire together. But neuroscientists in the 1990s discovered something more precise: it matters <em>when</em> they fire relative to each other.</p> <p>If neuron A fires just before neuron B suggesting A might have caused B to fire, the connection strengthens. But if A fires just after B, the connection weakens. This asymmetry, measured in milliseconds, is called <strong>Spike-Timing-Dependent Plasticity</strong> (Markram et al., 1997; Bi &amp; Poo, 1998).</p> <p>STDP is how your brain learns causation. It’s why you flinch at the sound of a dentist’s drill because the sound reliably preceded the discomfort. The brain encodes: this predicts that. It does this automatically, continuously, with extraordinary temporal precision.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/stdp.webp" width="400" height="auto" alt="STDP"/> </p> <p>This mechanism is also remarkably energy-efficient. Rather than computing global error signals across entire networks (as artificial systems do), biological neurons adjust locally based on timing alone. Evolution discovered a learning algorithm that runs on approximately 20 watts, the power of a dim light bulb, while performing learning, memory retrieval, sensory processing, and motor control simultaneously.</p> <h2 id="3-from-biology-to-computation">3. From Biology to Computation</h2> <p>By the mid 20th century, neuroscientists had revealed the brain’s learning mechanisms: Hebbian plasticity, LTP, LTD, timing-dependent rules. Engineers looked at this and asked an ambitious question: could we recreate this in computers?</p> <p>The honest answer is: not exactly. Simulating billions of neurons firing spikes with millisecond precision was computationally impossible in 1943. It’s still brutally expensive today. So instead of copying biology faithfully, engineers simplified. They kept the logic and discarded the chemistry.</p> <h3 id="the-first-artificial-neuron-1943">The First Artificial Neuron (1943)</h3> <p>Warren McCulloch (a neuroscientist) and Walter Pitts (a mathematician) proposed the first mathematical model of a neuron. Their “McCulloch-Pitts neuron” was a logic gate: it received inputs, summed them, and produced an output of 0 or 1 based on a threshold (McCulloch &amp; Pitts, 1943). No spikes and timing, just arithmetic.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/mc-neuron.webp" width="400" height="auto" alt="WM-Neuron"/> </p> <p>This brutal simplification preserved something essential: the idea that complex behavior could emerge from simple units connected in networks. That insight launched the field of artificial neural networks.</p> <h3 id="the-perceptron-a-neuron-that-learns-1958">The Perceptron: A Neuron That Learns (1958)</h3> <p>The McCulloch-Pitts neuron had fixed connections. It couldn’t adapt. In 1958, Frank Rosenblatt changed that with the <strong>Perceptron</strong>—an artificial neuron that could adjust its own connection strengths (weights) based on feedback (Rosenblatt, 1958). Show a Perceptron labeled images of triangles and squares; tell it when it’s wrong. Over time, it adjusts its weights until it classifies correctly.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/bio-vs-ai-neuron.webp" width="580" height="auto" alt="Perceptron"/> </p> <p>The Perceptron was revolutionary but limited. It could only learn patterns that were “linearly separable”, a restriction that seemed fatal when Minsky and Papert publicized it in 1969. AI winter descended.</p> <h3 id="backpropagation-ais-version-of-learning-1986">Backpropagation: AI’s Version of Learning (1986)</h3> <p>The comeback required solving a problem: how do you train a <em>network</em> of neurons, not just one? If a network with hidden layers produces the wrong answer, which connections are responsible? How do you distribute blame?</p> <p>The solution was <strong>Backpropagation</strong>, short for “backward propagation of errors.” Published in its modern form by Rumelhart, Hinton, and Williams in 1986, this algorithm calculates the gradient of error with respect to each weight in the network, then adjusts weights in the direction that reduces error (Rumelhart, Hinton, &amp; Williams, 1986).</p> <p>Here is the critical divergence between biology and AI:</p> <ul> <li><strong>Biology</strong> uses spike timing (STDP)—local, precise, energy-efficient.</li> <li><strong>AI</strong> uses gradient descent (Backpropagation)—global, mathematical, computationally expensive.</li> </ul> <p>We sacrificed biological realism for mathematical tractability. Backpropagation isn’t likely how brains learn. But it works and it scales.</p> <h3 id="the-transformer-attention-is-all-you-need-2017">The Transformer: Attention Is All You Need (2017)</h3> <p>For decades, neural networks processed information sequentially, one token at a time in order. This was slow and made it hard to capture long-range relationships in data.</p> <p>In 2017, a team at Google introduced the <strong>Transformer</strong> architecture, built on a mechanism called “self-attention” that allows the network to look at an entire input simultaneously and weigh the relevance of each part to every other part (Vaswani et al., 2017). Interestingly, this wasn’t entirely un-biological. Your visual cortex does something similar when you search for a friend in a crowd, your brain amplifies signals matching their face while dimming everything else (Desimone &amp; Duncan, 1995). This unlocked parallel processing at massive scale. Suddenly, training on billions of web pages became feasible.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/self_attn.svg" width="540" height="auto" alt="Self Attention"/> </p> <h3 id="large-language-models-compressing-human-knowledge-2020">Large Language Models: Compressing Human Knowledge (2020)</h3> <p>Combine Transformers with Backpropagation and unprecedented data. Train on billions of documents, books, websites, conversations. The result is models like GPT-3, which demonstrated that a neural network with 175B parameters could perform tasks it was never explicitly trained for: translation, coding, reasoning, even poetry (Brown et al., 2020).</p> <p>What’s happening inside these models? The weights, analogous to synaptic strengths—have encoded compressed statistical patterns from human generated text. They predict and autocomplete with such sophistication that the output often appears indistinguishable from human thought. We didn’t build a brain. We built a mathematical approximation massive enough to simulate one.</p> <h2 id="4-counter-intuitive-findings-in-memory-and-learning">4. Counter-Intuitive Findings in Memory and Learning</h2> <p>The story so far has traced a clean arc from biological neurons to artificial ones. But the brain holds deeper secret mechanisms so counter-intuitive they reshape how we should approach our own learning.</p> <h3 id="every-memory-is-a-reconstruction">Every Memory Is a Reconstruction</h3> <p>For most of the 20th century, scientists assumed memory worked like a file cabinet: you store a memory, and later you retrieve it unchanged. This assumption turned out to be fundamentally incorrect.</p> <p>In 2000, Karim Nader and colleagues demonstrated that every time you recall a memory, it becomes unstable and editable for approximately an hour. During this window, if you block certain proteins, the memory can be erased entirely. You don’t retrieve memories; you <em>reconstruct</em> them and each reconstruction can alter the original (Nader, Schafe, &amp; LeDoux, 2000).</p> <p>This is why eyewitness testimony is unreliable. Every time you tell a story from your past, you tell a slightly modified version of that, thus you’re not accessing a recording. You’re performing a creative act, subtly rewriting history each time.</p> <h3 id="teaching-others-teaches-you">Teaching Others Teaches You</h3> <p>Here’s a puzzle: you read a book and feel like you understand it. But when someone asks you to explain it, you stumble. Gaps appear that you didn’t know existed. This is the <strong>Protégé Effect</strong>. Fiorella and Mayer (2013) showed that students who studied material expecting to teach it outperformed those who studied to take a test. More remarkably, actually teaching, explaining aloud produced even greater gains.</p> <p>Why? Teaching forces <strong>linearization</strong>. Your understanding exists as a parallel, interconnected web of concepts. But speech is sequential, one word after another. To teach, you must translate that web into a chain. This process exposes gaps you can’t “speak around”. Teaching doesn’t just transmit knowledge. It debugs and consolidates the teacher’s own understanding.</p> <h3 id="curiosity-chemically-enhances-memory">Curiosity Chemically Enhances Memory</h3> <p>You remember interesting things better than boring ones. This isn’t just psychology; it’s neurochemistry. Gruber, Gelman, and Ranganath (2014) put people in brain scanners and induced states of high or low curiosity. When curiosity was high, the midbrain’s dopaminergic system became active—the same circuitry involved in reward and motivation. This dopamine surge enhanced activity in the hippocampus, the brain’s memory-formation center.</p> <p>Result: people remembered not only what they were curious about but also unrelated information encountered during curious states. Curiosity literally opens a neurochemical window for learning.</p> <h3 id="confusion-is-the-sweat-of-learning">Confusion Is the Sweat of Learning</h3> <p>Common intuition: if practice feels smooth, you’re learning well. If it feels hard, you’re doing something wrong. Research says the opposite. Kornell and Bjork (2008) demonstrated that <strong>interleaved practice</strong>-mixing different skills or topics randomly feels more difficult and frustrating than blocked practice (focusing on one thing at a time). But interleaved practice produces dramatically better long-term retention.</p> <p>The discomfort is the signal that learning is occurring. When everything feels easy, you’re likely just recognizing familiar patterns without building new understanding.</p> <h3 id="the-brain-that-works-when-you-dont">The Brain That Works When You Don’t</h3> <p>Stop focusing. Let your mind wander. You might think you’re being unproductive. You’re wrong. In 2001, Marcus Raichle discovered the <strong>Default Mode Network</strong> (DMN), a system of brain regions that becomes <em>more</em> active when you stop concentrating on external tasks (Raichle et al., 2001). The DMN is involved in self-reflection, future planning, and creative recombination of ideas.</p> <p>This is why insights often arrive in the shower or on walks. The brain has a “night shift” that connects disparate ideas—but it can only work when the “day shift” of focused attention clocks out.</p> <h3 id="sleep-is-ctrls">Sleep Is Ctrl+S</h3> <p>You practiced hard today. You feel like you’ve improved. But here’s the truth: the improvement isn’t consolidated until you sleep. During slow-wave sleep, the hippocampus “replays” the day’s learning at accelerated speed, transferring information to long-term storage in the neocortex (Stickgold, 2005). If you pull an all-nighter after learning something, the memory trace can physically dissolve. You don’t just forget; the learning evaporates as if it never happened.</p> <p align="center"> <img src="/assets/blogpost/01-2025-12-02/memory-formation.webp" width="500" height="auto" alt="Memory Formation"/> </p> <p>You cannot cram your way to lasting knowledge. Sleep is not optional; it is part of the learning process itself.</p> <h3 id="your-brain-grows-new-neurons-kills-them-if-you-dont-learn">Your Brain Grows New Neurons (Kills Them If You Don’t Learn)</h3> <p>For a century, the dogma held: you’re born with all the neurons you’ll ever have. Adults don’t grow new brain cells. Then came Eriksson et al. (1998), who proved that the human hippocampus generates new neurons throughout life, a process called <strong>adult neurogenesis</strong>. Thousands of neurons are born each day.</p> <p>But there’s a catch. These newborn neurons require a purpose. If they’re not recruited into learning new things within approximately two weeks, they die (Tashiro et al., 2007; Shors et al., 2012). Stress accelerates their death; learning and novelty help them survive. Your brain is constantly trying to upgrade its hardware. Whether those upgrades stick depends on whether you give them software to run.</p> <blockquote> <p><strong>Note:</strong> This finding remains contested. Sorrells et al. (2018) found sharply declining neurogenesis after childhood in humans, while Boldrini et al. (2018) found it persists. The debate is unresolved.</p> </blockquote> <h2 id="5-synthesis-biology-and-computation">5. Synthesis: Biology and Computation</h2> <p>We’ve traveled from synapses to computers and back. The question now is: what have we actually learned?</p> <h3 id="two-solutions-to-the-same-problem">Two Solutions to the Same Problem</h3> <p>Biology and engineering faced identical challenge, how do you build a system that improves with experience? Evolution solved it first through approximately 500 million years of trial and error, nervous systems developed Hebbian plasticity, LTP, LTD, and STDP. These mechanisms share a common design philosophy: local computation, temporal precision, and radical energy efficiency. A human brain runs on approximately 20 watts while performing tasks that require megawatts from data centers.</p> <p>Engineers, confronting the same problem in the 1940s, took a different path. Simulating biological neurons spike-by-spike was computationally intractable, so they abstracted. The McCulloch-Pitts neuron kept only the logic. The Perceptron added adaptability. Backpropagation distributed error globally rather than relying on local timing. Transformers abandoned sequence for parallel attention. Each step traded biological fidelity for mathematical scalability. The result is two fundamentally different architectures that both learn.</p> <h3 id="what-each-system-lacks">What Each System Lacks</h3> <p>Neither solution is complete. Large language models can generate text indistinguishable from human writing, translate between languages, write functional code, and even produce passable poetry. But they don’t “understand” in any phenomenological sense. They predict; they don’t comprehend. They have no persistent memory across conversations, no embodied experience, no stakes in being correct.</p> <p>Biological brains, by contrast, understand deeply. They ground knowledge in sensory experience, maintain continuity of self across decades, and care about outcomes. But they forget constantly, fatigue after hours of concentration, hold biases they cannot inspect, and die.</p> <p>Each system’s strengths illuminate the other’s gaps. AI researchers increasingly study neuroscience for architectural inspiration. Neuroscientists use artificial networks as testable models of biological computation. The fields are converging.</p> <h3 id="the-compression-principle">The Compression Principle</h3> <p>One framework connects both: the <strong>Information Bottleneck Theory</strong> (Tishby, Pereira, &amp; Bialek, 1999). It proposes that effective learning, whether biological or artificial, is fundamentally about compression. The goal isn’t to memorize everything; it’s to discard noise while preserving signal.</p> <p>Consider your memory of a 2,500-page book. You don’t retain every sentence. You extract characters, plot structure, key arguments, emotional peaks. This “lossy compression” isn’t a failure of memory, it’s the feature. Perfect recall would be metabolically expensive and cognitively useless. What matters is extracting structure that generalizes.</p> <p>The same principle governs neural networks. During training, deep networks progressively compress their internal representations, discarding input-specific details while preserving features relevant to the task. Tishby’s insight was that this compression is not incidental, it’s the mechanism by which generalization occurs.</p> <h3 id="why-teaching-works">Why Teaching Works</h3> <p>This framework also explains the Protégé Effect from Section 4. When you read a book, you compress it into a mental model, a parallel web of interconnected concepts. When you teach, you must decompress that model into sequential speech. This forced linearization exposes gaps. You discover what you thought you understood but actually stored at low resolution.</p> <p>The act of explaining forces re-encoding. You compress again, but this time with awareness of where the previous compression failed. Each cycle of compression and decompression increases fidelity. Teaching isn’t just transmission; it’s iterative refinement of your own representations.</p> <h3 id="the-ongoing-convergence">The Ongoing Convergence</h3> <p>We stand at an unusual moment in intellectual history. For the first time, humanity has built learning systems sophisticated enough to serve as working hypotheses for how biological cognition might operate, while simultaneously having neuroscientific tools precise enough to test those hypotheses against actual brains.</p> <p>The next decade will likely see these fields merge further. Neuromorphic chips already implement STDP in hardware. Transformer architectures are being reverse engineered to understand why they work, yielding insights that may apply to cortical computation. The boundary between “understanding the brain” and “building intelligence” is dissolving.</p> <p>What remains clear is that neither biology nor engineering has finished the job. Intelligence, in its fullest sense, remains an open problem on both fronts.</p> <h2 id="conclusion">Conclusion</h2> <p>Your brain is not a passive receiver of information. It is an active sculptor of neural tissue, strengthening some connections, pruning others, timing its adjustments with millisecond precision. When you learn something, your brain physically changes. When you sleep, it consolidates. When you teach, it debugs. When you’re curious, it amplifies.</p> <p>Artificial intelligence emerged from humanity’s attempt to understand and replicate these processes. We didn’t succeed in copying the brain, we succeeded in approximating its outcomes through different mathematics. The result is machines that can predict human language with unsettling accuracy, not because they think like us, but because they’ve been trained on everything we’ve ever written.</p> <p>The deeper lesson is this: understanding how learning works biologically and computationally, gives you leverage over your own cognition. Space your practice. Embrace confusion. Sleep. Teach what you want to master. Follow your curiosity, it’s not indulgence. It’s your dopamine system telling you where your brain is ready to grow.</p> <p>You are not a fixed entity. You are a process, one that rewires itself with every experience. Now you know the mechanisms. But consider the implications. If learning is compression, and memory is reconstruction, then your sense of understanding your own mind is itself a compressed, reconstructed narrative. When researchers surgically disconnected a patient’s left and right brain hemispheres and asked his left hemisphere why he was walking out of the room, it confidently replied, “I’m going to get a Coke”—even though only his right hemisphere knew the real reason (Gazzaniga &amp; LeDoux, 1978). The left hemisphere wasn’t lying. It was doing what brains always do: compressing incomplete information into a coherent story. If half your brain can fabricate explanations without knowing it’s confabulating, how much of what you “know” about your own cognition is actually true?</p> <h2 id="further-reading">Further Reading</h2> <p>For readers who want to go deeper, these books offer accessible yet rigorous explorations of the topics covered:</p> <ul> <li> <p>Kandel, E. R. (2006). <em>In Search of Memory: The Emergence of a New Science of Mind</em>.</p> </li> <li> <p>Eagleman, D. (2011). <em>Incognito: The Secret Lives of the Brain</em>.</p> </li> <li> <p>Carey, B. (2014). <em>How We Learn: The Surprising Truth About When, Where, and Why It Happens</em>.</p> </li> <li> <p>Russell, S. (2019). <em>Human Compatible: Artificial Intelligence and the Problem of Control</em>.</p> </li> <li> <p>Gazzaniga, M. S. (2018). <em>The Consciousness Instinct: Unraveling the Mystery of How the Brain Makes the Mind</em>.</p> </li> </ul> <h2 id="references">References</h2> <p>Bi, G., &amp; Poo, M. (1998). Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type. <em>Journal of Neuroscience</em>, 18(24), 10464–10472. https://doi.org/10.1523/JNEUROSCI.18-24-10464.1998</p> <p>Bliss, T. V. P., &amp; Lømo, T. (1973). Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. <em>The Journal of Physiology</em>, 232(2), 331–356. https://doi.org/10.1113/jphysiol.1973.sp010273</p> <p>Boldrini, M., Fulmore, C. A., Tartt, A. N., Simeon, L. R., Pavlova, I., Poposka, V., … &amp; Mann, J. J. (2018). Human hippocampal neurogenesis persists throughout aging. <em>Cell Stem Cell</em>, 22(4), 589–599. https://doi.org/10.1016/j.stem.2018.03.015</p> <p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Amodei, D. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, 33, 1877–1901. https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</p> <p>Desimone, R., &amp; Duncan, J. (1995). Neural mechanisms of selective visual attention. <em>Annual Review of Neuroscience</em>, 18, 193–222. https://doi.org/10.1146/annurev.ne.18.030195.001205</p> <p>Eriksson, P. S., Perfilieva, E., Björk-Eriksson, T., Alborn, A. M., Nordborg, C., Peterson, D. A., &amp; Gage, F. H. (1998). Neurogenesis in the adult human hippocampus. <em>Nature Medicine</em>, 4(11), 1313–1317. https://doi.org/10.1038/3305</p> <p>Fiorella, L., &amp; Mayer, R. E. (2013). The relative benefits of learning by teaching and teaching expectancy. <em>Contemporary Educational Psychology</em>, 38(4), 281–288. https://doi.org/10.1016/j.cedpsych.2013.06.001</p> <p>Gazzaniga, M. S., &amp; LeDoux, J. E. (1978). <em>The Integrated Mind</em>. Plenum Press.</p> <p>Gruber, M. J., Gelman, B. D., &amp; Ranganath, C. (2014). States of curiosity modulate hippocampus-dependent learning via the dopaminergic circuit. <em>Neuron</em>, 84(2), 486–496. https://doi.org/10.1016/j.neuron.2014.08.060</p> <p>Haier, R. J., Siegel, B. V., Nuechterlein, K. H., Hazlett, E., Wu, J. C., Paek, J., … &amp; Buchsbaum, M. S. (1988). Cortical glucose metabolic rate correlates of abstract reasoning and attention studied with positron emission tomography. <em>Intelligence</em>, 12(2), 199–217. https://doi.org/10.1016/0160-2896(88)90016-5</p> <p>Hebb, D. O. (1949). <em>The Organization of Behavior: A Neuropsychological Theory</em>. Wiley &amp; Sons.</p> <p>Ito, M., &amp; Kano, M. (1982). Long-lasting depression of parallel fiber-Purkinje cell transmission induced by conjunctive stimulation of parallel fibers and climbing fibers in the cerebellar cortex. <em>Neuroscience Letters</em>, 33(3), 253–258. https://doi.org/10.1016/0304-3940(82)90380-9</p> <p>Kornell, N., &amp; Bjork, R. A. (2008). Learning concepts and categories: Is spacing the “enemy of induction”? <em>Psychological Science</em>, 19(6), 585–592. https://doi.org/10.1111/j.1467-9280.2008.02127.x</p> <p>Markram, H., Lübke, J., Frotscher, M., &amp; Sakmann, B. (1997). Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs. <em>Science</em>, 275(5297), 213–215. https://doi.org/10.1126/science.275.5297.213</p> <p>McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. <em>Bulletin of Mathematical Biophysics</em>, 5, 115–133. https://doi.org/10.1007/BF02478259</p> <p>Miller, E. K., &amp; Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. <em>Annual Review of Neuroscience</em>, 24, 167–202. https://doi.org/10.1146/annurev.neuro.24.1.167</p> <p>Minsky, M., &amp; Papert, S. (1969). <em>Perceptrons: An Introduction to Computational Geometry</em>. MIT Press.</p> <p>Nader, K., Schafe, G. E., &amp; LeDoux, J. E. (2000). Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval. <em>Nature</em>, 406(6797), 722–726. https://doi.org/10.1038/35021052</p> <p>Raichle, M. E., MacLeod, A. M., Snyder, A. Z., Powers, W. J., Gusnard, D. A., &amp; Shulman, G. L. (2001). A default mode of brain function. <em>Proceedings of the National Academy of Sciences</em>, 98(2), 676–682. https://doi.org/10.1073/pnas.98.2.676</p> <p>Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological Review</em>, 65(6), 386–408. https://doi.org/10.1037/h0042519</p> <p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, 323(6088), 533–536. https://doi.org/10.1038/323533a0</p> <p>Shors, T. J., Anderson, M. L., Curlik, D. M., &amp; Nokia, M. S. (2012). Use it or lose it: How neurogenesis keeps the brain fit for learning. <em>Behavioural Brain Research</em>, 227(2), 450–458. https://doi.org/10.1016/j.bbr.2011.04.023</p> <p>Sorrells, S. F., Paredes, M. F., Cebrian-Silla, A., Sandoval, K., Qi, D., Kelley, K. W., … &amp; Bhattacharya, N. (2018). Human hippocampal neurogenesis drops sharply in children to undetectable levels in adults. <em>Nature</em>, 555(7696), 377–381. https://doi.org/10.1038/nature25975</p> <p>Stickgold, R. (2005). Sleep-dependent memory consolidation. <em>Nature</em>, 437(7063), 1272–1278. https://doi.org/10.1038/nature04286</p> <p>Tashiro, A., Makino, H., &amp; Gage, F. H. (2007). Experience-specific functional modification of the dentate gyrus through adult neurogenesis: A critical period during an immature stage. <em>Journal of Neuroscience</em>, 27(12), 3252–3259. https://doi.org/10.1523/JNEUROSCI.4970-06.2007</p> <p>Tishby, N., Pereira, F. C., &amp; Bialek, W. (1999). The information bottleneck method. In <em>Proceedings of the 37th Annual Allerton Conference on Communication, Control, and Computing</em> (pp. 368–377).</p> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, 30, 5998–6008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</p>]]></content><author><name></name></author><category term="article"/><category term="neuroscience"/><category term="ai"/><category term="learning"/><category term="memory"/><summary type="html"><![CDATA[This article examines the biological substrates of learning and memory, tracing their influence on the development of artificial neural networks.]]></summary></entry></feed>